{"version":3,"sources":["services/pose-util.js","components/pose/pose.js","components/pose/test.js","App.js","serviceWorker.js","index.js"],"names":["isMobile","test","navigator","userAgent","toTuple","y","x","drawPoint","ctx","r","color","beginPath","arc","Math","PI","fillStyle","fill","drawSegment","scale","ay","ax","by","bx","moveTo","lineTo","lineWidth","strokeStyle","stroke","drawSkeleton","keypoints","minConfidence","adjacentKeyPoints","posenet","forEach","position","drawKeypoints","i","length","keypoint","score","drawBoundingBox","boundingBox","rect","minX","minY","maxX","maxY","Stats","stats","setupCamera","a","mediaDevices","getUserMedia","Error","video","document","getElementById","width","height","mobile","facingMode","undefined","stream","srcObject","Promise","resolve","onloadedmetadata","loadVideo","play","guiState","algorithm","input","architecture","outputStride","inputResolution","multiplier","quantBytes","singlePoseDetection","minPoseConfidence","minPartConfidence","multiPoseDetection","maxPoseDetections","nmsRadius","output","showVideo","showSkeleton","showPoints","showBoundingBox","net","setupFPS","showPanel","appendChild","dom","detectPoseInRealTime","canvas","getContext","poseDetectionFrame","begin","poses","estimatePoses","flipHorizontal","decodingMethod","pose","concat","maxDetections","scoreThreshold","all_poses","console","log","clearRect","save","translate","drawImage","restore","end","requestAnimationFrame","info","textContent","style","display","webkitGetUserMedia","mozGetUserMedia","Test","useEffect","bindPage","id","playsInline","App","className","Boolean","window","location","hostname","match","ReactDOM","render","StrictMode","serviceWorker","ready","then","registration","unregister","catch","error","message"],"mappings":"yYAoCO,SAASA,IACd,MARO,WAAWC,KAAKC,UAAUC,YAI1B,oBAAoBF,KAAKC,UAAUC,WAwC5C,SAASC,EAAT,GACE,MAAO,CADgB,EAAPC,EAAO,EAAJC,GAId,SAASC,EAAUC,EAAKH,EAAGC,EAAGG,EAAGC,GACtCF,EAAIG,YACJH,EAAII,IAAIN,EAAGD,EAAGI,EAAG,EAAG,EAAII,KAAKC,IAC7BN,EAAIO,UAAYL,EAChBF,EAAIQ,OAMC,SAASC,EAAT,IAAyCP,EAAOQ,EAAOV,GAAM,IAAD,mBAAtCW,EAAsC,KAAlCC,EAAkC,wBAA5BC,EAA4B,KAAxBC,EAAwB,KACjEd,EAAIG,YACJH,EAAIe,OAAOH,EAAKF,EAAOC,EAAKD,GAC5BV,EAAIgB,OAAOF,EAAKJ,EAAOG,EAAKH,GAC5BV,EAAIiB,UAtEY,EAuEhBjB,EAAIkB,YAAchB,EAClBF,EAAImB,SAMC,SAASC,EAAaC,EAAWC,EAAetB,GAAiB,IAAZU,EAAW,uDAAH,EAC5Da,EACFC,IAA6BH,EAAWC,GAE5CC,EAAkBE,SAAQ,SAACJ,GACzBZ,EACIb,EAAQyB,EAAU,GAAGK,UAAW9B,EAAQyB,EAAU,GAAGK,UAtF/C,OAuFNhB,EAAOV,MAOR,SAAS2B,EAAcN,EAAWC,EAAetB,GACtD,IADuE,IAAZU,EAAW,uDAAH,EAC1DkB,EAAI,EAAGA,EAAIP,EAAUQ,OAAQD,IAAK,CACzC,IAAME,EAAWT,EAAUO,GAE3B,KAAIE,EAASC,MAAQT,GAArB,CAHyC,MAO1BQ,EAASJ,SAAjB7B,EAPkC,EAOlCA,EAAGC,EAP+B,EAO/BA,EACVC,EAAUC,EAAKH,EAAIa,EAAOZ,EAAIY,EAAO,EAvG3B,UAgHP,SAASsB,EAAgBX,EAAWrB,GACzC,IAAMiC,EAAcT,IAAuBH,GAE3CrB,EAAIkC,KACAD,EAAYE,KAAMF,EAAYG,KAAMH,EAAYI,KAAOJ,EAAYE,KACnEF,EAAYK,KAAOL,EAAYG,MAEnCpC,EAAIkB,YAtHmB,MAuHvBlB,EAAImB,SAgG6B3B,ICrOrB,IAAI+C,IDmOlB,IEjNMC,EAAQ,IAAID,I,SAMHE,I,2EAAf,gCAAAC,EAAA,yDACOhD,UAAUiD,cAAiBjD,UAAUiD,aAAaC,aADzD,sBAEU,IAAIC,MACN,iEAHR,cAMQC,EAAQC,SAASC,eAAe,UAChCC,MAfW,IAgBjBH,EAAMI,OAfY,IAiBZC,EAAS3D,IAVjB,SAWuBE,UAAUiD,aAAaC,aAAa,CACvD,OAAS,EACT,MAAS,CACPQ,WAAY,OACZH,MAAOE,OAASE,EAvBH,IAwBbH,OAAQC,OAASE,EAvBH,OAOpB,cAWQC,EAXR,OAmBER,EAAMS,UAAYD,EAnBpB,kBAqBS,IAAIE,SAAQ,SAACC,GAClBX,EAAMY,iBAAmB,WACvBD,EAAQX,QAvBd,6C,+BA4Bea,I,2EAAf,4BAAAjB,EAAA,sEACsBD,IADtB,cACQK,EADR,QAEQc,OAFR,kBAISd,GAJT,4C,sBAOA,IAMMe,EAAW,CACfC,UAAW,aACXC,MAAO,CACLC,aAAc,cACdC,aAP2B,GAQ3BC,gBAPoC,IAQpCC,WAV+B3E,IAAa,GAAO,IAWnD4E,WAbsB,GAexBC,oBAAqB,CACnBC,kBAAmB,GACnBC,kBAAmB,IAErBC,mBAAoB,CAClBC,kBAAmB,EACnBH,kBAAmB,IACnBC,kBAAmB,GACnBG,UAAW,IAEbC,OAAQ,CACNC,WAAW,EACXC,cAAc,EACdC,YAAY,EACZC,iBAAiB,GAEnBC,IAAK,MAWP,SAASC,IACPzC,EAAM0C,UAAU,GAChBnC,SAASC,eAAe,QAAQmC,YAAY3C,EAAM4C,KAOpD,SAASC,EAAqBvC,EAAOkC,GACnC,IAAMM,EAASvC,SAASC,eAAe,UACjChD,EAAMsF,EAAOC,WAAW,MAFU,SAazBC,IAbyB,2EAaxC,oCAAA9C,EAAA,sDAGEF,EAAMiD,QAEFC,EAAQ,GALd,KAQU7B,EAASC,UARnB,OASS,gBATT,OAkBS,eAlBT,wCAUyBD,EAASmB,IAAIW,cAAc7C,EAAO,CACnD8C,gBAhBmB,EAiBnBC,eAAgB,kBAZxB,cAUYC,EAVZ,OAcMJ,EAAQA,EAAMK,OAAOD,GACrBxB,GAAqBT,EAASQ,oBAAoBC,kBAClDC,GAAqBV,EAASQ,oBAAoBE,kBAhBxD,8CAmB4BV,EAASmB,IAAIW,cAAc7C,EAAO,CACtD8C,gBAzBmB,EA0BnBC,eAAgB,eAChBG,cAAenC,EAASW,mBAAmBC,kBAC3CwB,eAAgBpC,EAASW,mBAAmBD,kBAC5CG,UAAWb,EAASW,mBAAmBE,YAxB/C,eAmBUwB,EAnBV,OA2BMR,EAAQA,EAAMK,OAAOG,GACrB5B,GAAqBT,EAASW,mBAAmBF,kBACjDC,GAAqBV,EAASW,mBAAmBD,kBA7BvD,6BAiCE4B,QAAQC,IAAIV,GAEZ1F,EAAIqG,UAAU,EAAG,EA9IF,IACC,KA+IZxC,EAASc,OAAOC,YAClB5E,EAAIsG,OACJtG,EAAIU,OAAO,EAAG,GACdV,EAAIuG,WAnJS,IAmJc,GAC3BvG,EAAIwG,UAAU1D,EAAO,EAAG,EApJX,IACC,KAoJd9C,EAAIyG,WAMNf,EAAMjE,SAAQ,YAAyB,IAAvBM,EAAsB,EAAtBA,MAAOV,EAAe,EAAfA,UACjBU,GAASuC,IACPT,EAASc,OAAOG,YAClBnD,EAAcN,EAAWkD,EAAmBvE,GAE1C6D,EAASc,OAAOE,cAClBzD,EAAaC,EAAWkD,EAAmBvE,GAEzC6D,EAASc,OAAOI,iBAClB/C,EAAgBX,EAAWrB,OAMjCwC,EAAMkE,MACNC,sBAAsBnB,GAhExB,6CAbwC,sBAUxCF,EAAOrC,MAxGU,IAyGjBqC,EAAOpC,OAxGW,IA6KlBsC,I,4CAOK,gCAAA9C,EAAA,sEAEalB,IAAa,CAC7BwC,aAAcH,EAASE,MAAMC,aAC7BC,aAAcJ,EAASE,MAAME,aAC7BC,gBAAiBL,EAASE,MAAMG,gBAChCC,WAAYN,EAASE,MAAMI,WAC3BC,WAAYP,EAASE,MAAMK,aAPxB,cAECY,EAFD,yBAcWrB,IAdX,OAcHb,EAdG,8DAgBC8D,EAAO7D,SAASC,eAAe,SAC9B6D,YAAc,oFAEnBD,EAAKE,MAAMC,QAAU,QAnBlB,aAsBLlD,EAASmB,IAAMA,EAEfC,IACAI,EAAqBvC,GAzBhB,0D,sBA4BPpD,UAAUkD,aAAelD,UAAUkD,cAC/BlD,UAAUsH,oBAAsBtH,UAAUuH,gBAuB/BC,MAnBf,WAII,OAHAC,qBAAU,YAlCP,WAAP,wBAmCQC,MAGA,yBAAKC,GAAG,QAAR,OAEI,2BACIpE,MA9NG,IA+NHC,OA9NI,IA+NJmE,GAAG,QACHP,MAAO,CAACC,QAAS,QACjBO,aAAW,IAEf,4BAAQD,GAAG,aC5ORE,MATf,WACE,OACE,yBAAKC,UAAU,OAEb,kBAAC,EAAD,QCGcC,QACW,cAA7BC,OAAOC,SAASC,UAEe,UAA7BF,OAAOC,SAASC,UAEhBF,OAAOC,SAASC,SAASC,MACvB,2DCZNC,IAASC,OACP,kBAAC,IAAMC,WAAP,KACE,kBAAC,EAAD,OAEFjF,SAASC,eAAe,SDyHpB,kBAAmBtD,WACrBA,UAAUuI,cAAcC,MACrBC,MAAK,SAAAC,GACJA,EAAaC,gBAEdC,OAAM,SAAAC,GACLpC,QAAQoC,MAAMA,EAAMC,c","file":"static/js/main.10654963.chunk.js","sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as posenet from '@tensorflow-models/posenet';\nimport * as tf from '@tensorflow/tfjs';\n\nconst color = 'aqua';\nconst boundingBoxColor = 'red';\nconst lineWidth = 2;\n\nexport const tryResNetButtonName = 'ds';\nexport const tryResNetButtonText = '[New] Try ResNenmnnt50';\nconst tryResNetButtonTextCss = 'width:100%;text-decoration:underline;';\nconst tryResNetButtonBackgroundCss = 'background:#e61d5f;';\n\nfunction isAndroid() {\n  return /Android/i.test(navigator.userAgent);\n}\n\nfunction isiOS() {\n  return /iPhone|iPad|iPod/i.test(navigator.userAgent);\n}\n\nexport function isMobile() {\n  return isAndroid() || isiOS();\n}\n\nfunction setDatGuiPropertyCss(propertyText, liCssString, spanCssString = '') {\n  var spans = document.getElementsByClassName('property-name');\n  for (var i = 0; i < spans.length; i++) {\n    var text = spans[i].textContent || spans[i].innerText;\n    if (text == propertyText) {\n      spans[i].parentNode.parentNode.style = liCssString;\n      if (spanCssString !== '') {\n        spans[i].style = spanCssString;\n      }\n    }\n  }\n}\n\nexport function updateTryResNetButtonDatGuiCss() {\n  setDatGuiPropertyCss(\n      tryResNetButtonText, tryResNetButtonBackgroundCss,\n      tryResNetButtonTextCss);\n}\n\n/**\n * Toggles between the loading UI and the main canvas UI.\n */\nexport function toggleLoadingUI(\n    showLoadingUI, loadingDivId = 'loading', mainDivId = 'main') {\n  if (showLoadingUI) {\n    document.getElementById(loadingDivId).style.display = 'block';\n    document.getElementById(mainDivId).style.display = 'none';\n  } else {\n    document.getElementById(loadingDivId).style.display = 'none';\n    document.getElementById(mainDivId).style.display = 'block';\n  }\n}\n\nfunction toTuple({y, x}) {\n  return [y, x];\n}\n\nexport function drawPoint(ctx, y, x, r, color) {\n  ctx.beginPath();\n  ctx.arc(x, y, r, 0, 2 * Math.PI);\n  ctx.fillStyle = color;\n  ctx.fill();\n}\n\n/**\n * Draws a line on a canvas, i.e. a joint\n */\nexport function drawSegment([ay, ax], [by, bx], color, scale, ctx) {\n  ctx.beginPath();\n  ctx.moveTo(ax * scale, ay * scale);\n  ctx.lineTo(bx * scale, by * scale);\n  ctx.lineWidth = lineWidth;\n  ctx.strokeStyle = color;\n  ctx.stroke();\n}\n\n/**\n * Draws a pose skeleton by looking up all adjacent keypoints/joints\n */\nexport function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {\n  const adjacentKeyPoints =\n      posenet.getAdjacentKeyPoints(keypoints, minConfidence);\n\n  adjacentKeyPoints.forEach((keypoints) => {\n    drawSegment(\n        toTuple(keypoints[0].position), toTuple(keypoints[1].position), color,\n        scale, ctx);\n  });\n}\n\n/**\n * Draw pose keypoints onto a canvas\n */\nexport function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {\n  for (let i = 0; i < keypoints.length; i++) {\n    const keypoint = keypoints[i];\n\n    if (keypoint.score < minConfidence) {\n      continue;\n    }\n\n    const {y, x} = keypoint.position;\n    drawPoint(ctx, y * scale, x * scale, 3, color);\n  }\n}\n\n/**\n * Draw the bounding box of a pose. For example, for a whole person standing\n * in an image, the bounding box will begin at the nose and extend to one of\n * ankles\n */\nexport function drawBoundingBox(keypoints, ctx) {\n  const boundingBox = posenet.getBoundingBox(keypoints);\n\n  ctx.rect(\n      boundingBox.minX, boundingBox.minY, boundingBox.maxX - boundingBox.minX,\n      boundingBox.maxY - boundingBox.minY);\n\n  ctx.strokeStyle = boundingBoxColor;\n  ctx.stroke();\n}\n\n/**\n * Converts an arary of pixel data into an ImageData object\n */\nexport async function renderToCanvas(a, ctx) {\n  const [height, width] = a.shape;\n  const imageData = new ImageData(width, height);\n\n  const data = await a.data();\n\n  for (let i = 0; i < height * width; ++i) {\n    const j = i * 4;\n    const k = i * 3;\n\n    imageData.data[j + 0] = data[k + 0];\n    imageData.data[j + 1] = data[k + 1];\n    imageData.data[j + 2] = data[k + 2];\n    imageData.data[j + 3] = 255;\n  }\n\n  ctx.putImageData(imageData, 0, 0);\n}\n\n/**\n * Draw an image on a canvas\n */\nexport function renderImageToCanvas(image, size, canvas) {\n  canvas.width = size[0];\n  canvas.height = size[1];\n  const ctx = canvas.getContext('2d');\n\n  ctx.drawImage(image, 0, 0);\n}\n\n/**\n * Draw heatmap values, one of the model outputs, on to the canvas\n * Read our blog post for a description of PoseNet's heatmap outputs\n * https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\n */\nexport function drawHeatMapValues(heatMapValues, outputStride, canvas) {\n  const ctx = canvas.getContext('2d');\n  const radius = 5;\n  const scaledValues = heatMapValues.mul(tf.scalar(outputStride, 'int32'));\n\n  drawPoints(ctx, scaledValues, radius, color);\n}\n\n/**\n * Used by the drawHeatMapValues method to draw heatmap points on to\n * the canvas\n */\nfunction drawPoints(ctx, points, radius, color) {\n  const data = points.buffer().values;\n\n  for (let i = 0; i < data.length; i += 2) {\n    const pointY = data[i];\n    const pointX = data[i + 1];\n\n    if (pointX !== 0 && pointY !== 0) {\n      ctx.beginPath();\n      ctx.arc(pointX, pointY, radius, 0, 2 * Math.PI);\n      ctx.fillStyle = color;\n      ctx.fill();\n    }\n  }\n}\n\n/**\n * Draw offset vector values, one of the model outputs, on to the canvas\n * Read our blog post for a description of PoseNet's offset vector outputs\n * https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\n */\n// export function drawOffsetVectors(\n//     heatMapValues, offsets, outputStride, scale = 1, ctx) {\n//   const offsetPoints =\n//       posenet.singlePose.getOffsetPoints(heatMapValues, outputStride, offsets);\n\n//   const heatmapData = heatMapValues.buffer().values;\n//   const offsetPointsData = offsetPoints.buffer().values;\n\n//   for (let i = 0; i < heatmapData.length; i += 2) {\n//     const heatmapY = heatmapData[i] * outputStride;\n//     const heatmapX = heatmapData[i + 1] * outputStride;\n//     const offsetPointY = offsetPointsData[i];\n//     const offsetPointX = offsetPointsData[i + 1];\n\n//     drawSegment(\n//         [heatmapY, heatmapX], [offsetPointY, offsetPointX], color, scale, ctx);\n//   }\n// }\n\n\nconst defaultQuantBytes = 2;\n\nconst defaultMobileNetMultiplier = isMobile() ? 0.50 : 0.75;\nconst defaultMobileNetStride = 16;\nconst defaultMobileNetInputResolution = 30;\n\nexport const guiState = {\n    algorithm: 'multi-pose',\n    input: {\n      architecture: 'MobileNetV1',\n      outputStride: defaultMobileNetStride,\n      inputResolution: defaultMobileNetInputResolution,\n      multiplier: defaultMobileNetMultiplier,\n      quantBytes: defaultQuantBytes\n    },\n    singlePoseDetection: {\n      minPoseConfidence: 0.1,\n      minPartConfidence: 0.5,\n    },\n    multiPoseDetection: {\n      maxPoseDetections: 5,\n      minPoseConfidence: 0.15,\n      minPartConfidence: 0.1,\n      nmsRadius: 30.0,\n    },\n    output: {\n      showVideo: true,\n      showSkeleton: true,\n      showPoints: true,\n      showBoundingBox: false,\n    },\n    net: null,\n  };","import React, { useState, useEffect, useRef } from 'react';\nimport * as posenet from '@tensorflow-models/posenet';\nimport Stats from 'stats.js';\nimport {guiState, drawBoundingBox, drawKeypoints, drawSkeleton, isMobile, toggleLoadingUI, tryResNetButtonName, tryResNetButtonText, updateTryResNetButtonDatGuiCss} from './../../services/pose-util';\nconst videoWidth = 600;\nconst videoHeight = 500;\nconst stats = new Stats();\n\n\nconst Pose = () => {\n    let videoRef = useRef();\n    let outputRef = useRef();\n    useEffect(()=> {\n        bindPage();\n    });\n    const [toggleUILoading, setToggleUILoading] = useState(true);\n\n    const setupFPS = () => {\n        stats.showPanel(0);  // 0: fps, 1: ms, 2: mb, 3+: custom\n        document.getElementById('main').appendChild(stats.dom);\n    }\n\n    /**\n        * Loads a the camera to be used in the demo\n        *\n    */\n    const setupCamera = async () => {\n        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {\n        throw new Error(\n            'Browser API navigator.mediaDevices.getUserMedia not available');\n        }\n    \n        const video = videoRef.current;\n        video.width = videoWidth;\n        video.height = videoHeight;\n    \n        const mobile = isMobile();\n        const stream = await navigator.mediaDevices.getUserMedia({\n        'audio': false,\n        'video': {\n            facingMode: 'user',\n            width: mobile ? undefined : videoWidth,\n            height: mobile ? undefined : videoHeight,\n        },\n        });\n        video.srcObject = stream;\n    \n        return new Promise((resolve) => {\n        video.onloadedmetadata = () => {\n            resolve(video);\n        };\n        });\n    }\n  \n    const loadVideo = async () => {\n        const video = await setupCamera();\n        video.play();\n    \n        return video;\n    }\n\n\n    const detectPoseInRealTime = async(video, net) => {\n        const canvas = outputRef.current;\n        const ctx = canvas.getContext('2d');\n        const flipPoseHorizontal = true;\n        \n        canvas.width = videoWidth;\n        canvas.height = videoHeight;\n\n        const poseDetectionFrame = async () => {\n            stats.begin();\n        \n            let poses = [];\n            let minPoseConfidence;\n            let minPartConfidence;\n            switch (guiState.algorithm) {\n            case 'single-pose':\n                const pose = await guiState.net.estimatePoses(video, {\n                    flipHorizontal: flipPoseHorizontal,\n                    decodingMethod: 'single-person'\n                });\n                poses = poses.concat(pose);\n                minPoseConfidence = +guiState.singlePoseDetection.minPoseConfidence;\n                minPartConfidence = +guiState.singlePoseDetection.minPartConfidence;\n                break;\n            case 'multi-pose':\n                let all_poses = await guiState.net.estimatePoses(video, {\n                flipHorizontal: flipPoseHorizontal,\n                decodingMethod: 'multi-person',\n                maxDetections: guiState.multiPoseDetection.maxPoseDetections,\n                scoreThreshold: guiState.multiPoseDetection.minPartConfidence,\n                nmsRadius: guiState.multiPoseDetection.nmsRadius\n                });\n        \n                poses = poses.concat(all_poses);\n                minPoseConfidence = +guiState.multiPoseDetection.minPoseConfidence;\n                minPartConfidence = +guiState.multiPoseDetection.minPartConfidence;\n                break;\n            }\n        \n            console.log(poses, video);\n        \n            ctx.clearRect(0, 0, videoWidth, videoHeight);\n        \n            if (guiState.output.showVideo) {\n            ctx.save();\n            ctx.scale(-1, 1);\n            ctx.translate(-videoWidth, 0);\n            ctx.drawImage(video, 0, 0, videoWidth, videoHeight);\n            ctx.restore();\n            }\n        \n            // For each pose (i.e. person) detected in an image, loop through the poses\n            // and draw the resulting skeleton and keypoints if over certain confidence\n            // scores\n            poses.forEach(({score, keypoints}) => {\n            if (score >= minPoseConfidence) {\n                if (guiState.output.showPoints) {\n                drawKeypoints(keypoints, minPartConfidence, ctx);\n                }\n                if (guiState.output.showSkeleton) {\n                drawSkeleton(keypoints, minPartConfidence, ctx);\n                }\n                if (guiState.output.showBoundingBox) {\n                drawBoundingBox(keypoints, ctx);\n                }\n            }\n            });\n        \n            // End monitoring code for frames per second\n            stats.end();\n        \n            requestAnimationFrame(poseDetectionFrame);\n        }\n        \n        poseDetectionFrame();\n    }\n\n\n    const bindPage = async () => {\n        setToggleUILoading(true);\n        const net = await posenet.load({\n            architecture: guiState.input.architecture,\n            outputStride: guiState.input.outputStride,\n            inputResolution: guiState.input.inputResolution,\n            multiplier: guiState.input.multiplier,\n            quantBytes: guiState.input.quantBytes\n        });\n        setToggleUILoading(false);\n\n        let video;\n        \n        try {\n            video = await loadVideo();\n        } catch (e) {\n            console.log(e);\n            return;\n            let info = document.getElementById('info');\n            info.textContent = 'this browser does not support video capture,' +\n                'or this device does not have a camera';\n            info.style.display = 'block';\n            throw e;\n        }\n        // guiState.net = net;\n        // setupGui([], net);\n        setupFPS();\n        detectPoseInRealTime(video, net);\n    }\n    return (\n        <div id='main'>\n            Pose\n            {!toggleUILoading && (\n                 <video \n                    width={videoWidth} \n                    height={videoHeight} \n                    ref={videoRef} \n                    playsInline  />\n            )}\n           \n            <canvas ref={outputRef} />\n        </div>\n    );\n}\n\nexport default Pose;","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as posenet from '@tensorflow-models/posenet';\nimport Stats from 'stats.js';\nimport React, { useEffect } from 'react';\n\nimport {drawBoundingBox, drawKeypoints, drawSkeleton, isMobile, toggleLoadingUI, tryResNetButtonName, tryResNetButtonText, updateTryResNetButtonDatGuiCss} from './../../services/pose-util';\n\nconst videoWidth = 600;\nconst videoHeight = 500;\nconst stats = new Stats();\n\n/**\n * Loads a the camera to be used in the demo\n *\n */\nasync function setupCamera() {\n  if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {\n    throw new Error(\n        'Browser API navigator.mediaDevices.getUserMedia not available');\n  }\n\n  const video = document.getElementById('video');\n  video.width = videoWidth;\n  video.height = videoHeight;\n\n  const mobile = isMobile();\n  const stream = await navigator.mediaDevices.getUserMedia({\n    'audio': false,\n    'video': {\n      facingMode: 'user',\n      width: mobile ? undefined : videoWidth,\n      height: mobile ? undefined : videoHeight,\n    },\n  });\n  video.srcObject = stream;\n\n  return new Promise((resolve) => {\n    video.onloadedmetadata = () => {\n      resolve(video);\n    };\n  });\n}\n\nasync function loadVideo() {\n  const video = await setupCamera();\n  video.play();\n\n  return video;\n}\n\nconst defaultQuantBytes = 2;\n\nconst defaultMobileNetMultiplier = isMobile() ? 0.50 : 0.75;\nconst defaultMobileNetStride = 16;\nconst defaultMobileNetInputResolution = 500;\n\nconst guiState = {\n  algorithm: 'multi-pose',\n  input: {\n    architecture: 'MobileNetV1',\n    outputStride: defaultMobileNetStride,\n    inputResolution: defaultMobileNetInputResolution,\n    multiplier: defaultMobileNetMultiplier,\n    quantBytes: defaultQuantBytes\n  },\n  singlePoseDetection: {\n    minPoseConfidence: 0.1,\n    minPartConfidence: 0.5,\n  },\n  multiPoseDetection: {\n    maxPoseDetections: 5,\n    minPoseConfidence: 0.15,\n    minPartConfidence: 0.1,\n    nmsRadius: 30.0,\n  },\n  output: {\n    showVideo: true,\n    showSkeleton: true,\n    showPoints: true,\n    showBoundingBox: false,\n  },\n  net: null,\n};\n\n/**\n * Sets up dat.gui controller on the top-right of the window\n */\n\n\n/**\n * Sets up a frames per second panel on the top-left of the window\n */\nfunction setupFPS() {\n  stats.showPanel(0);  // 0: fps, 1: ms, 2: mb, 3+: custom\n  document.getElementById('main').appendChild(stats.dom);\n}\n\n/**\n * Feeds an image to posenet to estimate poses - this is where the magic\n * happens. This function loops with a requestAnimationFrame method.\n */\nfunction detectPoseInRealTime(video, net) {\n  const canvas = document.getElementById('output');\n  const ctx = canvas.getContext('2d');\n\n  // since images are being fed from a webcam, we want to feed in the\n  // original image and then just flip the keypoints' x coordinates. If instead\n  // we flip the image, then correcting left-right keypoint pairs requires a\n  // permutation on all the keypoints.\n  const flipPoseHorizontal = true;\n\n  canvas.width = videoWidth;\n  canvas.height = videoHeight;\n\n  async function poseDetectionFrame() {\n\n    // Begin monitoring code for frames per second\n    stats.begin();\n\n    let poses = [];\n    let minPoseConfidence;\n    let minPartConfidence;\n    switch (guiState.algorithm) {\n      case 'single-pose':\n        const pose = await guiState.net.estimatePoses(video, {\n          flipHorizontal: flipPoseHorizontal,\n          decodingMethod: 'single-person'\n        });\n        poses = poses.concat(pose);\n        minPoseConfidence = +guiState.singlePoseDetection.minPoseConfidence;\n        minPartConfidence = +guiState.singlePoseDetection.minPartConfidence;\n        break;\n      case 'multi-pose':\n        let all_poses = await guiState.net.estimatePoses(video, {\n          flipHorizontal: flipPoseHorizontal,\n          decodingMethod: 'multi-person',\n          maxDetections: guiState.multiPoseDetection.maxPoseDetections,\n          scoreThreshold: guiState.multiPoseDetection.minPartConfidence,\n          nmsRadius: guiState.multiPoseDetection.nmsRadius\n        });\n\n        poses = poses.concat(all_poses);\n        minPoseConfidence = +guiState.multiPoseDetection.minPoseConfidence;\n        minPartConfidence = +guiState.multiPoseDetection.minPartConfidence;\n        break;\n    }\n\n    console.log(poses);\n\n    ctx.clearRect(0, 0, videoWidth, videoHeight);\n\n    if (guiState.output.showVideo) {\n      ctx.save();\n      ctx.scale(-1, 1);\n      ctx.translate(-videoWidth, 0);\n      ctx.drawImage(video, 0, 0, videoWidth, videoHeight);\n      ctx.restore();\n    }\n\n    // For each pose (i.e. person) detected in an image, loop through the poses\n    // and draw the resulting skeleton and keypoints if over certain confidence\n    // scores\n    poses.forEach(({score, keypoints}) => {\n      if (score >= minPoseConfidence) {\n        if (guiState.output.showPoints) {\n          drawKeypoints(keypoints, minPartConfidence, ctx);\n        }\n        if (guiState.output.showSkeleton) {\n          drawSkeleton(keypoints, minPartConfidence, ctx);\n        }\n        if (guiState.output.showBoundingBox) {\n          drawBoundingBox(keypoints, ctx);\n        }\n      }\n    });\n\n    // End monitoring code for frames per second\n    stats.end();\n    requestAnimationFrame(poseDetectionFrame);\n  }\n\n  poseDetectionFrame();\n}\n\n/**\n * Kicks off the demo by loading the posenet model, finding and loading\n * available camera devices, and setting off the detectPoseInRealTime function.\n */\nexport async function bindPage() {\n//   toggleLoadingUI(true);\n  const net = await posenet.load({\n    architecture: guiState.input.architecture,\n    outputStride: guiState.input.outputStride,\n    inputResolution: guiState.input.inputResolution,\n    multiplier: guiState.input.multiplier,\n    quantBytes: guiState.input.quantBytes\n  });\n//   toggleLoadingUI(false);\n\n  let video;\n\n  try {\n    video = await loadVideo();\n  } catch (e) {\n    let info = document.getElementById('info');\n    info.textContent = 'this browser does not support video capture,' +\n        'or this device does not have a camera';\n    info.style.display = 'block';\n    throw e;\n  }\n  guiState.net = net;\n  // setupGui([], net);\n  setupFPS();\n  detectPoseInRealTime(video, net);\n}\n\nnavigator.getUserMedia = navigator.getUserMedia ||\n    navigator.webkitGetUserMedia || navigator.mozGetUserMedia;\n// kick off the demo\n\n\nfunction Test () {\n    useEffect(() => {\n        bindPage();\n    })\n    return (\n        <div id='main'>\n            Pose\n            <video \n                width={videoWidth} \n                height={videoHeight} \n                id=\"video\" \n                style={{display: 'none'}}\n                playsInline  />\n           \n            <canvas id='output' />\n        </div>\n    )\n}\n\nexport default Test;\n","import React from 'react';\nimport './App.css';\nimport Pose from './components/pose/pose';\nimport Test from './components/pose/test';\n\nfunction App() {\n  return (\n    <div className=\"App\">\n      {/* <Pose /> */}\n      <Test />\n    </div>\n  );\n}\n\nexport default App;\n","// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.0/8 are considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\nexport function register(config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl, config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl, config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl, {\n    headers: { 'Service-Worker': 'script' },\n  })\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready\n      .then(registration => {\n        registration.unregister();\n      })\n      .catch(error => {\n        console.error(error.message);\n      });\n  }\n}\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport * as serviceWorker from './serviceWorker';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.unregister();\n"],"sourceRoot":""}